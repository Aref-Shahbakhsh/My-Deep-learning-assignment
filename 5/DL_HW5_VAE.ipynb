{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSEBrPlLYS_9"
      },
      "source": [
        "#  CE-40710: Deep Learning\n",
        "#  Fall 2023 - Dr. Beigy "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nq5dlNliOvR8"
      },
      "source": [
        "## Homework 5: VAE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4Yuka_u595a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FzWeSbVf6PN6"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chXkHOC16R-y"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, classification_report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iUrebfa-TUc"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abFG0W_D6TnY"
      },
      "outputs": [],
      "source": [
        "CUDA = True\n",
        "BATCH_SIZE = 128\n",
        "LEARNING_RATE = 0.01"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhGLLFe66ZW5"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WNptQFm-UoA"
      },
      "outputs": [],
      "source": [
        "# Reproducibility options\n",
        "\n",
        "np.random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "\n",
        "if CUDA:\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1icSKPPO6ab5"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8zcZ6dy6XLC"
      },
      "outputs": [],
      "source": [
        "# MNIST Dataset\n",
        "original_train_dataset = datasets.MNIST(root='./mnist_data/', train=True, transform=transforms.ToTensor(), download=True)\n",
        "original_test_dataset = datasets.MNIST(root='./mnist_data/', train=False, transform=transforms.ToTensor(), download=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BvLDysfd7iPk"
      },
      "outputs": [],
      "source": [
        "# Define Train loader\n",
        "train_tensors = original_train_dataset.data.float() / 255\n",
        "test_tensors = original_test_dataset.data.float() / 255\n",
        "\n",
        "# Define Datasets\n",
        "train_dataset = torch.utils.data.TensorDataset(train_tensors, original_train_dataset.targets)\n",
        "test_dataset = torch.utils.data.TensorDataset(test_tensors, original_test_dataset.targets)\n",
        "\n",
        "# Define dataloaders\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEPkUvsIP8LC"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InbXIT_NP9uK"
      },
      "outputs": [],
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(self, input_dim, encoder_dims, z_dim, decoder_dims, dropout_rate=0.2):\n",
        "        super(VAE, self).__init__()\n",
        "        \n",
        "        self.input_dim = input_dim\n",
        "        self.z_dim = z_dim\n",
        "        \n",
        "        ########################## TODO ##########################\n",
        "        # self.input_dropout must be a dropout module with p=dropout_rate\n",
        "        pass\n",
        "        ######################## END TODO ########################\n",
        "        \n",
        "        # Encoder part\n",
        "        encoder_layers = []\n",
        "        ########################## TODO ##########################\n",
        "        # Define encoder layers and add them to `encoder_layers`\n",
        "        # Use ReLU for activation functions\n",
        "        pass\n",
        "        ######################## END TODO ########################\n",
        "        self.encoder = nn.Sequential(*encoder_layers)\n",
        "        \n",
        "        encoder_last_dim = ([input_dim] + encoder_dims)[-1]\n",
        "        \n",
        "        # mu and log_var\n",
        "        ########################## TODO ##########################\n",
        "        # Define mu and log_var layers\n",
        "        # They do not need any activation function\n",
        "        pass\n",
        "        ######################## END TODO ########################\n",
        "        \n",
        "        # Decoder part\n",
        "        decoder_layers = []\n",
        "        ########################## TODO ##########################\n",
        "        # Define decoder layers and add them to `decoder_layers`\n",
        "        # Use ReLU for activation functions\n",
        "        # Last layer does not need any activation function\n",
        "        pass\n",
        "        ######################## END TODO ########################\n",
        "        self.decoder = nn.Sequential(*decoder_layers)\n",
        "\n",
        "    @staticmethod\n",
        "    def _sampling(mu, log_var):\n",
        "        \"\"\"\n",
        "        This function is in charge of reparametrization trick\n",
        "        \"\"\"\n",
        "        ########################## TODO ##########################\n",
        "        # Generate a random z from N(mu=mu, var=e^log_var)\n",
        "        pass\n",
        "        ######################## END TODO ########################\n",
        "        return z\n",
        "\n",
        "    def forward(self, data):\n",
        "        x = data.get(\"X\")\n",
        "        ########################## TODO ##########################\n",
        "        # Complete the flow\n",
        "        # x > dropout > encoder > mu, log_var > z > decoder > output\n",
        "        pass\n",
        "        ######################## END TODO ########################\n",
        "        return {\"X\": output, \"mu\": mu, \"log_var\": log_var, \"z\": z}\n",
        "    \n",
        "    def loss(self, data, output):\n",
        "      x = data.get(\"X\")\n",
        "      recon_x = output.get(\"X\")\n",
        "      mu = output.get(\"mu\")\n",
        "      log_var = output.get(\"log_var\")\n",
        "      ########################## TODO ##########################\n",
        "      # Calculate MSE and KLD\n",
        "      pass\n",
        "      ######################## END TODO ########################\n",
        "      return {\"final\": MSE + KLD, \"KLD\": KLD, \"MSE\": MSE}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZML_5ruP8V0"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VR0_6oqA_eoI"
      },
      "outputs": [],
      "source": [
        "def train(model, data_loader, optimizer, epoch=\"?\", print_every=100, verbose=True):\n",
        "    \"\"\"\n",
        "    This function trains a `model` given a `data_loader` by `optimizer`\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "\n",
        "    train_loss = 0\n",
        "\n",
        "    for batch_idx, batch_data in enumerate(data_loader):\n",
        "        supervised_training = len(batch_data) == 2\n",
        "\n",
        "        if supervised_training:\n",
        "            # Labeled data\n",
        "            data = {\"X\": batch_data[0],\n",
        "                    \"Y\": batch_data[1]}\n",
        "        else:\n",
        "            # Unlabeled data\n",
        "            data = {\"X\": batch_data[0]}\n",
        "\n",
        "        # Flatten data\n",
        "        ########################## TODO ##########################\n",
        "        # Use `flatten` function from pytorch\n",
        "        pass\n",
        "        ######################## END TODO ########################\n",
        "\n",
        "        if CUDA:\n",
        "            ########################## TODO ##########################\n",
        "            # Send tensors in data to GPU\n",
        "            pass\n",
        "            ######################## END TODO ########################\n",
        "\n",
        "\n",
        "        ########################## TODO ##########################\n",
        "        # Think about this part\n",
        "        ######################## END TODO ########################\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(data)\n",
        "        loss = model.loss(data, output)\n",
        "        final_loss = loss[\"final\"]\n",
        "        train_loss += final_loss.item()\n",
        "\n",
        "        final_loss.backward()\n",
        "        optimizer.step()\n",
        "        ########################## TODO ##########################\n",
        "        # Thinking ends here\n",
        "        ######################## END TODO ########################\n",
        "\n",
        "        if batch_idx % print_every == 0 and verbose:\n",
        "            print('Train | Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data[\"X\"]), len(data_loader.dataset),\n",
        "                100. * batch_idx / len(data_loader), final_loss.item() / len(data[\"X\"])))\n",
        "\n",
        "    train_loss /= len(data_loader.dataset)\n",
        "    if verbose:\n",
        "        print('====> Train | Epoch: {} \\t | \\t Average loss: {:.4f}'.format(epoch, train_loss,))\n",
        "\n",
        "    return {\"final\": train_loss}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "el0PNTe8CUoh"
      },
      "outputs": [],
      "source": [
        "def test(model, data_loader, verbose=True):\n",
        "    \"\"\"\n",
        "    This function tests a `model` on a `data_loader`\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    test_loss = 0\n",
        "\n",
        "    labels, y_preds = None, None\n",
        "    with torch.no_grad():\n",
        "        for batch_data  in data_loader:\n",
        "            supervised = len(batch_data) == 2\n",
        "\n",
        "            if supervised:\n",
        "                # Labeled data\n",
        "                data = {\"X\": batch_data[0],\n",
        "                        \"Y\": batch_data[1]}\n",
        "            else:\n",
        "                # Unlabeled data\n",
        "                data = {\"X\": batch_data[0]}\n",
        "\n",
        "            #  Flatten data\n",
        "            ########################## TODO ##########################\n",
        "            # Use `flatten` function from pytorch\n",
        "            pass\n",
        "            ######################## END TODO ########################\n",
        "\n",
        "            if CUDA:\n",
        "                ########################## TODO ##########################\n",
        "                # Send tensors in data to GPU\n",
        "                pass\n",
        "                ######################## END TODO ########################\n",
        "\n",
        "            output = model(data)\n",
        "            loss = model.loss(data, output)\n",
        "            final_loss = loss[\"final\"]\n",
        "            test_loss += final_loss.item()\n",
        "\n",
        "            # Storing y_true and y_pred if learning is supervised\n",
        "            supervised = supervised and \"Y\" in output\n",
        "            if supervised:\n",
        "                y_pred = np.argmax(output[\"Y\"].detach().cpu().numpy(), axis=1)\n",
        "                y_true = data[\"Y\"].cpu().numpy()\n",
        "                if labels is None:\n",
        "                    labels = y_true\n",
        "                    y_preds = y_pred\n",
        "                else:\n",
        "                    labels = np.concatenate((labels, y_true), axis=None)\n",
        "                    y_preds = np.concatenate((y_preds, y_pred), axis=None)\n",
        "\n",
        "    test_loss /= len(data_loader.dataset)\n",
        "\n",
        "    if supervised:\n",
        "        accuracy = accuracy_score(labels, y_preds)\n",
        "        if verbose:\n",
        "            print('====> Test  | model loss: {:.4f}'.format(test_loss))\n",
        "            print('            |   accuracy: {:.4f}'.format(100 * accuracy))\n",
        "            print(classification_report(labels, y_preds))\n",
        "        return {\"final\": test_loss, \"accuracy\": accuracy}\n",
        "    else:\n",
        "        if verbose:\n",
        "            print('====> Test  | model loss: {:.4f}'.format(test_loss))\n",
        "        return {\"final\": test_loss}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPQtOVFm-7Ng"
      },
      "outputs": [],
      "source": [
        "def run(model, parameters, n_epoch, train_loader, test_loader,\n",
        "        end_function=lambda model:None):\n",
        "    \"\"\"\n",
        "    This function will optimize `parameters` of `model` for `n_epoch` epochs\n",
        "    on `train_loader` dataset and validate it on `test_loader`.\n",
        "    At the end of each epoch, `end_function` will be called on `model`.\n",
        "    \"\"\"\n",
        "    if CUDA:\n",
        "        model.cuda()\n",
        "\n",
        "    ########################## TODO ##########################\n",
        "    # Initialize a new Adam optimizer.\n",
        "    # Set learning rate to LEARNING_RATE / BATCH_SIZE (why?)\n",
        "    pass\n",
        "    ######################## END TODO ########################\n",
        "\n",
        "    for epoch in range(1, n_epoch + 1):\n",
        "        train_loss = train(model, train_loader, optimizer, epoch)\n",
        "        test_loss = test(model, test_loader)\n",
        "        end_function(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_K3w1YkLHiKF"
      },
      "outputs": [],
      "source": [
        "def show(image_batch, rows=1):\n",
        "    # Set Plot dimensions\n",
        "    cols = np.ceil(image_batch.shape[0] / rows)\n",
        "    plt.rcParams['figure.figsize'] = (0.0 + cols, 0.0 + rows) # set default size of plots\n",
        "    \n",
        "    for i in range(image_batch.shape[0]):\n",
        "        plt.subplot(rows, cols, i + 1)\n",
        "        plt.imshow(image_batch[i], cmap=\"gray\", vmin=0, vmax=1)\n",
        "        plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Bt0iPlDo7Os"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qe0EBQP_dJf6"
      },
      "outputs": [],
      "source": [
        "vae = VAE(784, encoder_dims=[512, 128], z_dim=32, decoder_dims=[128, 512])\n",
        "print(vae)\n",
        "\n",
        "# Output might be like:\n",
        "# VAE(\n",
        "#   (input_dropout): Dropout(p=0.2)\n",
        "#   (encoder): Sequential(\n",
        "#     (0): Linear(in_features=784, out_features=512, bias=True)\n",
        "#     (1): ReLU()\n",
        "#     (2): Linear(in_features=512, out_features=128, bias=True)\n",
        "#     (3): ReLU()\n",
        "#   )\n",
        "#   (mu_layer): Linear(in_features=128, out_features=32, bias=True)\n",
        "#   (log_var_layer): Linear(in_features=128, out_features=32, bias=True)\n",
        "#   (decoder): Sequential(\n",
        "#     (0): Linear(in_features=32, out_features=128, bias=True)\n",
        "#     (1): ReLU()\n",
        "#     (2): Linear(in_features=128, out_features=512, bias=True)\n",
        "#     (3): ReLU()\n",
        "#     (4): Linear(in_features=512, out_features=784, bias=True)\n",
        "#   )\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XU1nDMGsS6Mn"
      },
      "outputs": [],
      "source": [
        "def show_vae(vae):\n",
        "    # Select 10 random inputs\n",
        "    data_loader = train_loader\n",
        "    indices = torch.randint(data_loader.dataset.tensors[0].shape[0], (10,))\n",
        "    input_img = data_loader.dataset.tensors[0][indices]\n",
        "    print(\"Inputs:\")\n",
        "    show(input_img.numpy())\n",
        "    # Calculate reconstructions\n",
        "    input_img = input_img.flatten(start_dim=1)\n",
        "    if CUDA:\n",
        "        input_img = input_img.cuda()\n",
        "    recons_img = vae({\"X\": input_img})[\"X\"]\n",
        "    recons_img = recons_img.detach().cpu().view(-1, 28, 28).numpy()\n",
        "    print(\"Reconstructions:\")\n",
        "    show(recons_img)\n",
        "    # Generate images\n",
        "    z = torch.randn(10, vae.z_dim)\n",
        "    if CUDA:\n",
        "        z = z.cuda()\n",
        "    generated_img = vae.decoder(z)\n",
        "    generated_img = generated_img.detach().cpu().view(-1, 28, 28).numpy()\n",
        "    print(\"Generated Images:\")\n",
        "    show(generated_img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxmmuAzv0JSH"
      },
      "outputs": [],
      "source": [
        "n_epoch = 30\n",
        "run(vae, vae.parameters(), n_epoch, train_loader, test_loader,\n",
        "   end_function=show_vae)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-ByVMHduz6j"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpkuO7zZQ8Op"
      },
      "outputs": [],
      "source": [
        "def check_interpolations(model, n_rows, n_cols):\n",
        "    \"\"\"\n",
        "    Write a function which interpolates 10 images between two random mnist image\n",
        "    \"\"\"\n",
        "    plt.rcParams['figure.figsize'] = (0.0 + n_cols, 0.0 + n_rows) # set default size of plots\n",
        "    \n",
        "    data_loader = train_loader\n",
        "    \n",
        "    for _ in range(n_rows):\n",
        "        indices = torch.randint(data_loader.dataset.tensors[0].shape[0], (2,))\n",
        "        img_1 = data_loader.dataset.tensors[0][indices][:1].flatten(start_dim=1)\n",
        "        img_2 = data_loader.dataset.tensors[0][indices][1:].flatten(start_dim=1)\n",
        "        \n",
        "        ########################## TODO ##########################\n",
        "        # Output `n_cols` images.\n",
        "        # First one should be img_1. Last one should by img_2\n",
        "        # Let z_1 and z_2 be latent tensors of img_1 and img_2\n",
        "        # Interpolate (n_cols - 2) tensors between z_1 and z_2\n",
        "        # Intermediate images must be the result of applying decoder on these (n_cols - 2) tensors\n",
        "        pass\n",
        "        ######################## END TODO ########################\n",
        "        \n",
        "        show(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FR7QvTv80bbF"
      },
      "outputs": [],
      "source": [
        "check_interpolations(vae, 20, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-e_NcAGX5P-W"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}