{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import os\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms"
      ],
      "metadata": {
        "id": "B1SOEhDb0dzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1- Download Dataset(2 p)\n",
        "download dataset from this link and extract it:\n",
        "http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/maps.tar.gz"
      ],
      "metadata": {
        "id": "FzhWJzxtH7uQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUqN5j2aH6r7"
      },
      "outputs": [],
      "source": [
        "# -----------------------------------------------------\n",
        "# Downaload dataset from above link with !wget\n",
        "# Extract it with \n",
        "# -----------------------------------------------------\n",
        "############### Complete this part ##############(2 points)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XAavQ-cgJq3k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2- Dataset Class(3 p)"
      ],
      "metadata": {
        "id": "n3Rd6cL9LCkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Maps_Dataset(Dataset):\n",
        "    def __init__(self, ImagesDirectory):\n",
        "        self.ImagesDirectory = ImagesDirectory\n",
        "        self.files = os.listdir(self.ImagesDirectory)\n",
        "        self.transforms = transforms.Compose([  transforms.Resize((256, 256), Image.BICUBIC),\n",
        "                                                transforms.ToTensor(),\n",
        "                                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # reading one image from dataset\n",
        "        img_name = self.files[index]\n",
        "        img_path = os.path.join(self.ImagesDirectory, img_name)\n",
        "        img = Image.open(img_path)\n",
        "        # -----------------------------------------------------\n",
        "        # split image into two part IMAGE and MAP (see one sample of dataset)\n",
        "        # apply self.transforms to IMAGE and MAP to Normalize and Resize the images\n",
        "        # -----------------------------------------------------\n",
        "        ############### Complete this part ############### (3 points)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        return {\"IMAGE\": IMAGE , \"MAP\": MAP}"
      ],
      "metadata": {
        "id": "U5V1otAcIRgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3-Create Generator with the help of the Block module and test it(32 p)"
      ],
      "metadata": {
        "id": "9DeeEUksLN8d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, down=True, act=\"relu\", use_dropout=False):\n",
        "        super(Block, self).__init__()\n",
        "\n",
        "        # Select appropriate convolutional or transpose convolutional layer\n",
        "        if down:\n",
        "            self.conv = nn.Conv2d(in_channels, out_channels, 4, 2, 1, bias=False, padding_mode=\"reflect\")\n",
        "        else:\n",
        "            self.conv = nn.ConvTranspose2d(in_channels, out_channels, 4, 2, 1, bias=False)\n",
        "        \n",
        "        # Initialize batch normalization and activation layers\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.act = nn.ReLU() if act == \"relu\" else nn.LeakyReLU(0.2)\n",
        "\n",
        "        # Flag for using dropout\n",
        "        self.use_dropout = use_dropout\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply convolutional layer\n",
        "        x = self.conv(x)\n",
        "\n",
        "        # Apply batch normalization and activation\n",
        "        x = self.bn(x)\n",
        "        x = self.act(x)\n",
        "\n",
        "        # Apply dropout if specified\n",
        "        if self.use_dropout:\n",
        "            x = self.dropout(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, in_channels=3):\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "        # -----------------------------------------------------\n",
        "        # based on aritcle appendix define Encoder( page 16)\n",
        "        # C64-C128-C256-C512-C512-C512-C512-C512\n",
        "        # -----------------------------------------------------\n",
        "        ############### Complete this part ############### (10 points)\n",
        "        self.initial_down = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 64 , 4, 2, 1, padding_mode=\"reflect\"),\n",
        "            nn.LeakyReLU(0.2),\n",
        "        ) # C3 => C64\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        self.bottleneck = nn.Sequential(\n",
        "            nn.Conv2d(512 , 512 , 4, 2, 1), nn.ReLU()\n",
        "        )\n",
        "        # -----------------------------------------------------\n",
        "        # based on aritcle appendix define Decoder ( page 16)\n",
        "        # CD512-CD512-CD512-C512-C256-C128-C64\n",
        "        # -----------------------------------------------------\n",
        "        ############### Complete this part ############### (10 points)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        self.final_up = nn.Sequential(\n",
        "            nn.ConvTranspose2d(128, in_channels, kernel_size=4, stride=2, padding=1),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # -----------------------------------------------------\n",
        "        # create network like U-Net\n",
        "        # -----------------------------------------------------\n",
        "        ############### Complete this part ############### (10 points)\n",
        "\n",
        "\n",
        "        return Out\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iJyfl8ulLNTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test():\n",
        "    # -----------------------------------------------------\n",
        "    #  test generator : output should be (B,C,256,256)\n",
        "    # -----------------------------------------------------\n",
        "    ############### Complete this part ############### (2 points)\n",
        "    # test the model\n",
        "\n",
        "\n",
        "test()"
      ],
      "metadata": {
        "id": "mnN6YiZ0LrB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4-Discriminator, Just test it (3 p)\n",
        "This code defines a discriminator network for a patch GAN. The network takes in two images of size 256x256 and concatenates them along the channel dimension. It then passes the concatenated image through a series of convolutional layers, batch normalization layers, and activation layers. The final layer is a convolutional layer with a single channel output image of size 32x32 each number shows the probability of being fake for a patch of the input image\n",
        "\n",
        "![images.jpg](data:image/jpeg;base64,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)"
      ],
      "metadata": {
        "id": "bvyTlgqfQ_uk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, in_channels=3, features=[64, 128, 256, 512]):\n",
        "        super().__init__()\n",
        "        # Initial convolutional layer with stride 2\n",
        "        self.initial = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels * 2,  # Number of input channels is doubled because of concatenation\n",
        "                features[0],  # Number of output channels\n",
        "                kernel_size=4,  # Kernel size\n",
        "                stride=2,  # Stride of 2 halves the size of the image\n",
        "                padding=1,  # Padding to maintain spatial dimensions\n",
        "                padding_mode=\"reflect\",  # Reflect padding pads the input with a reflection of itself\n",
        "            ),\n",
        "            nn.LeakyReLU(0.2),  # Activation function\n",
        "        )\n",
        "\n",
        "        # Sequential layer with multiple convolutional and normalization layers\n",
        "        layers = []\n",
        "        in_channels = features[0]  # Number of input channels for the first layer\n",
        "        for feature in features[1:]:\n",
        "            layers.append(\n",
        "                nn.Sequential(\n",
        "                    nn.Conv2d(\n",
        "                        in_channels,  # Number of input channels\n",
        "                        feature,  # Number of output channels\n",
        "                        kernel_size=4,  # Kernel size\n",
        "                        stride=1 if feature == features[-1] else 2,  # Stride of 1 for the last layer, otherwise 2\n",
        "                        padding=1,  # Padding to maintain spatial dimensions\n",
        "                        padding_mode=\"reflect\",  # Reflect padding pads the input with a reflection of itself\n",
        "                    ),\n",
        "                    nn.BatchNorm2d(feature),  # Batch normalization layer\n",
        "                    nn.LeakyReLU(0.2),  # Activation function\n",
        "                )\n",
        "            )\n",
        "            in_channels = feature  # Update the number of input channels for the next layer\n",
        "\n",
        "        # Final convolutional layer with single channel output\n",
        "        layers.append(\n",
        "            nn.Conv2d(\n",
        "                in_channels,  # Number of input channels\n",
        "                1,  # Number of output channels\n",
        "                kernel_size=4,  # Kernel size\n",
        "                stride=1,  # Stride of 1 maintains the size of the image\n",
        "                padding=2,  # Padding to maintain spatial dimensions\n",
        "                padding_mode=\"reflect\",  # Reflect padding pads the input with a reflection of itself\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        # Stack the layers into a single model\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        # Concatenate the two input images along the channel dimension\n",
        "        x = torch.cat([x, y], dim=1)\n",
        "        # Pass the concatenated image through the initial convolutional layer\n",
        "        x = self.initial(x)\n",
        "        # Pass the output through the rest of the layers\n",
        "        x = self.model(x)\n",
        "        # Return the output\n",
        "        return x"
      ],
      "metadata": {
        "id": "gQKWQFA7UyBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_discriminator():\n",
        "        # -----------------------------------------------------\n",
        "        #  test discriminator : output should be (B,1,32,32)\n",
        "        # -----------------------------------------------------\n",
        "        ############### Complete this part ############### (3 points)\n",
        "\n",
        "\n",
        "\n",
        "test_discriminator()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jsTYh-LKVLLJ",
        "outputId": "5c5e8a6c-6702-4bea-fa04-3b0a1f141166"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1, 32, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5-define hyperparameters and useful functions"
      ],
      "metadata": {
        "id": "Vrf8RGNzfhki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_SET_TRAIN = \"/content/maps/train\"\n",
        "DATA_SET_VAL = \"/content/maps/val\"\n",
        "BATCH_SIZE = 1\n",
        "LR = 0.0002\n",
        "B1 = 0.5\n",
        "B2 = 0.999\n",
        "IMG_HEIGHT = 256\n",
        "IMG_WIDTH =  256\n",
        "\n",
        "# This function initializes the weights of a neural network model in PyTorch. It takes a single argument, m, which is a module (e.g., a layer) in the model.\n",
        "# This function is often used as a weights_init function when defining a PyTorch model, which can then be applied to the model using the apply method:\n",
        "# model = MyModel()\n",
        "# model.apply(weights_init_normal)\n",
        "\n",
        "\n",
        "def weights_init_normal(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find(\"Conv\") != -1:\n",
        "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find(\"BatchNorm2d\") != -1:\n",
        "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        torch.nn.init.constant_(m.bias.data, 0.0)"
      ],
      "metadata": {
        "id": "nF1-ZFzkfg_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6-training(50 p)"
      ],
      "metadata": {
        "id": "K5-qmZRejzf9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Check if GPU is available and store the result in 'cuda'\n",
        "cuda = torch.cuda.is_available()\n",
        "\n",
        "# GAN loss function using mean squared error\n",
        "criterion_GAN = torch.nn.MSELoss()\n",
        "\n",
        "# Pixel-wise loss function using mean absolute error\n",
        "criterion_pixelwise = torch.nn.L1Loss()\n",
        "\n",
        "# Weight for pixel-wise loss in total loss\n",
        "lambda_pixel = 100\n",
        "\n",
        "# Initialize generator and discriminator networks\n",
        "generator = Generator()\n",
        "discriminator = Discriminator()\n",
        "\n",
        "# Initialize network weights using normal distribution\n",
        "generator.apply(weights_init_normal)\n",
        "discriminator.apply(weights_init_normal)\n",
        "\n",
        "# If GPU is available, move the networks and loss functions to the GPU\n",
        "if cuda:\n",
        "    generator = generator.cuda()\n",
        "    discriminator = discriminator.cuda()\n",
        "    criterion_GAN.cuda()\n",
        "    criterion_pixelwise.cuda()\n",
        "\n",
        "# Adam optimizers for generator and discriminator\n",
        "optimizer_G = torch.optim.Adam(generator.parameters(), lr=LR, betas=(B1, B2))\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=LR, betas=(B1, B2))\n",
        "\n",
        "# DataLoader for training data\n",
        "dataloader = DataLoader(Maps_Dataset(DATA_SET_TRAIN), batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "\n",
        "# DataLoader for validation data\n",
        "val_dataloader = DataLoader(Maps_Dataset(DATA_SET_VAL), batch_size=4, shuffle=True, num_workers=2)\n",
        "\n",
        "# Tensor type for GPU or CPU\n",
        "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(EPOCH):\n",
        "    for i, batch in enumerate(dataloader):\n",
        "\n",
        "        \n",
        "        # Get model inputs\n",
        "        IMG_REAL = Variable(batch[\"IMAGE\"].type(Tensor))\n",
        "        MAP_REAL = Variable(batch[\"MAP\"].type(Tensor))\n",
        "\n",
        "        # Adversarial ground truths\n",
        "        patcc_gan_output = (1, 32,32)\n",
        "        REAL = Variable(Tensor(np.ones((IMG_REAL.size(0), *patcc_gan_output))), requires_grad=False)\n",
        "        FAKE = Variable(Tensor(np.zeros((IMG_REAL.size(0), *patcc_gan_output))), requires_grad=False)\n",
        "\n",
        "        # -------------------------------------------------------\n",
        "        #  Train Generator\n",
        "        # -------------------------------------------------------\n",
        "        ############### Complete this part ############### (15 points)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # ------------------------------------------------------\n",
        "        #  Train Discriminator\n",
        "        # ------------------------------------------------------\n",
        "        ############### Complete this part ############### (15 points)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        Iteration = epoch * len(dataloader) + i\n",
        "\n",
        "        # -----------------------------------------------------\n",
        "        #  every 500 iterations: print logs you need \n",
        "        # -----------------------------------------------------\n",
        "        ############### Complete this part ############### (10 points)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        #------------------------------------------------------\n",
        "        # every 500 iterations: give one batch of val data(4 images) to generator and save results in .png or .jpg\n",
        "        #------------------------------------------------------\n",
        "        ############### Complete this part ############### (10 points)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nP4cOdVsjIWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Display (10 P)\n"
      ],
      "metadata": {
        "id": "FnPy9q1gpUjM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#------------------------------------------------------\n",
        "# display 12 (3 batches of the best results you saved in the previous step)\n",
        "#------------------------------------------------------\n",
        "############### Complete this part ############### (10 points)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KE8OpYMGpUG6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}